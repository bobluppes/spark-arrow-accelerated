# Map Spark SQL queries to FPGA building blocks

> :warning: This project is in a very early stage and currently the goal is to understand the involved technologies. I aim to build a minimal runnable solution at first. Therefore the code is very volatile and is not handling different situations.

## Goal

* Mapping of SparkSQL queries to FPGA building blocks by using [Fletcher](https://github.com/abs-tudelft/fletcher) & [Appache Arrow](https://github.com/apache/arrow)

## Archived Steps

* Replace the code generated by Spark with own code (very simple & dirty for now)
* Convert Spark partition to Apache Arrow Vector
* Store Apache Arrow Vector in Plasma Store
* Call native code via JNI
* calculate sum of Arrow Vector in native code

## Build 

1. [Build Appache Arrow](https://arrow.apache.org/docs/developers/cpp/building.html) with the options `DARROW_PLASMA=ON` and `DARROW_PLASMA_JAVA_CLIENT=ON`
2. Set the root dir of the arrow project as env variable `export ARROW_ROOT=/path/to/arrow`
3. [Start the plasma server](https://github.com/apache/arrow/blob/master/cpp/apidoc/tutorials/plasma.md) e.g. `./plasma-store-server -m 1000000000 -s /tmp/plasma`
4. Run the gradle build (including tests) `./gradlew build`

## Project structure

* `spark-extension` this code integrates with spark and is replacing the default strategy of generating the execution plan. Instead it converts the data of a partition into the Apache Arrow format and forwards it.
* `arrow-processor` scala part which forward the Arrow vector to the native code bw writing it into the Plasma store
* `arrow-processor-native` code who is responsible for the actual execution of the processing steps. For now this is trivial and it just sums the elements
